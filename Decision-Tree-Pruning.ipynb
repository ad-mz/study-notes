{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff3ce980",
   "metadata": {},
   "source": [
    "## Decision Tree - Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36388ff5",
   "metadata": {},
   "source": [
    "<img src=\"img/linkedin.png\" width=\"15\"/> [LinkedIn - How to prune a decision tree](https://www.linkedin.com/learning/machine-learning-with-python-decision-trees/how-to-prune-a-decision-tree?autoSkip=true&resume=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0407d244",
   "metadata": {},
   "source": [
    "A decision tree that **overfits** does a great job explaining the data in the training set but performs poorly when given new or test data. To avoid overfitting, we have to carefully manage the size of a decision tree during or after the recursive partitioning process. this process is known as **pruning**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2f0d4",
   "metadata": {},
   "source": [
    "Pruning helps a decision tree to generalize well, which means that it will likely perform well when presented with previously unseen data. We can limit the size of a decision tree during the recursive partitioning process by setting criteria that need to be met at each split point. These criteria can be in the form of the maximum number of features to be considered for each split, the maximum number of decision nodes to allow for the tree or the minimum number of data points to allow in each partition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c80555",
   "metadata": {},
   "source": [
    "This approach to pruning is known as **pre-pruning**. Pre-pruning is appealing in that prevents unnecessary branches and nodes from being created, which saves compute cycles. However, pre-pruning could stop tree growth too early. This means that the tree may not get a chance to learn certain patterns in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eaeb80",
   "metadata": {},
   "source": [
    "An alternative pruning approach is **post-pruning**. As the name suggests, the idea here is to allow the decision tree to grow as large as it can and then reduce its size afterward. With regards to compute time, post-pruning is not as efficient as pre-pruning, however, it does provide the benefits of being more effective in discovering important patterns within the data. There are several approaches to post-pruning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7297a72",
   "metadata": {},
   "source": [
    "One of the most popular is known as **cost complexity pruning or weakest link pruning**. To help explain our cost complexity pruning works, let's imagine that we work for a placement agency and that we just received the results of an income survey conducted by our agency. Each survey response includes the age of the worker, their level of education or highest degree earned and their annual salary. \n",
    "\n",
    "<img src=\"img/regressionPruning_01.png\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab6b926",
   "metadata": {},
   "source": [
    "Limiting our focus to age and annual salary, each of the survey responses can be represented on a scatterplot this way. Recall that the idea behind recursive partitioning is to repeatedly split data into smaller subsets in such a way that maximizes the homogeneity or similarity of items within each subset. \n",
    "\n",
    "If we allow the recursive partitioning process to continue uninhibited on this data, it would partition on every possible age value and we would end up with partitions that look like this. \n",
    "\n",
    "<img src=\"img/regressionPruning_02.png\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ddbca5",
   "metadata": {},
   "source": [
    "This would result in a decision tree that looked like this. This decision tree fits our data too perfectly and has likely overfit. \n",
    "\n",
    "<img src=\"img/regressionPruning_03.png\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8beee7c",
   "metadata": {},
   "source": [
    "A better decision tree for our data is one that is based on fewer than the maximum number of partitions, like one of these shown here. We can think of these sub-trees of pruned version of the previous one.  \n",
    "\n",
    "<img src=\"img/regressionPruning_04.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f89a3f",
   "metadata": {},
   "source": [
    "The question then is how do we choose the best sub-tree to use? This is where **cost complexity pruning** comes in. The first step in the process is to calculate the sum of squared residuals or SSR for each sub-tree. Notice that the larger the tree gets, the lower its SSR. This means that the larger the tree is, the better job it does at explaining the relationship between the independent variables and the dependent variable. However, we also know that the larger the tree is, the more likely it is to overfit against our data. \n",
    "\n",
    "<img src=\"img/regressionPruning_05.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bbffe7",
   "metadata": {},
   "source": [
    "More info on how to compute the SSR for the tree\n",
    "[Decision Tree - SSR](Decision-Tree-SSR.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcb4f10",
   "metadata": {},
   "source": [
    "To account for this, a better metric for which sub-tree to choose is the **tree score** or **cost complexity measure**. The cost complexity measure for a tree is the sum of squared residuals for the tree plus the **tree complexity penalty**. The tree complexity penalty compensates for the number of leaf nodes in the tree. It is a product of the complexity parameter, a user-defined parameter and the number of leaf nodes in the tree. As a side note, because the decision tree used in this illustration is a regression tree, we use the sum of squared residuals in calculating the tree score. If we were working with a **classification tree**, we would use **entropy** or **gini** instead of SSR to calculate the tree score. \n",
    "\n",
    "<img src=\"img/regressionPruning_06.png\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fbfb78",
   "metadata": {},
   "source": [
    "If we set the alpha, the complexity parameter, to 1,000, the tree score for each sub-tree will be as shown here. This means that we will choose the first sub-tree because it has the lowest tree score. As you may have noticed, the choice of alpha has a significant impact on which tree ends up having the lowest tree score. \n",
    "\n",
    "<img src=\"img/regressionPruning_07.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4303feb",
   "metadata": {},
   "source": [
    "For example, if we reduce the value of alpha to 100, we get different tree scores for each tree. This means that we would choose this sub-tree instead because it now has the lowest tree score. Higher values for alpha favor simpler trees, while smaller values for alpha favor more complex trees. \n",
    "\n",
    "<img src=\"img/regressionPruning_08.png?v=1\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412b0c59",
   "metadata": {},
   "source": [
    "The question then is how do we decide on a value for **alpha**? The simple answer is that alpha is a **hyperparameter** and we use a process known as **hyperparameter tuning** to find the best value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7ff03b",
   "metadata": {},
   "source": [
    "Conceptually, finding the best alpha value involves training several sub-trees based on different values for alpha, then choosing the sub-tree that performs the best on the test data. \n",
    "\n",
    "<img src=\"img/regressionPruning_09.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8928a9b1",
   "metadata": {},
   "source": [
    "The alpha for this tree is the best value for alpha.\n",
    "\n",
    "<img src=\"img/regressionPruning_10.png\" width=\"500px\">"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
