{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f46fb71",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008d1d08",
   "metadata": {},
   "source": [
    "## What is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d970aa12",
   "metadata": {},
   "source": [
    "A decision tree is a machine learning approach that uses an inverted tree-like structure to model the relationship between independent variables and a dependent variable. Decision trees are a supervised machine learning approach. This means that we can use a decision tree to predict future outcomes. \n",
    "\n",
    "Because of their versatility and ease of use, decision trees are one of the most widely used machine learning approaches. Just like a tree has a trunk which is connected to leaves by branches, a decision tree is a collection of decision nodes which are connected to other decision nodes or leaf nodes by branches. \n",
    "\n",
    "We can also think of a decision tree as a collection of questions, responses, and outcomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e4593d",
   "metadata": {},
   "source": [
    "To illustrate this point, let's assume we are thinking of getting a new job.  \n",
    "For us, the choice of whether or not to accept a new job offer hinges on certain key considerations. Let's assume that the most important consideration is salary, specifically we only accept a job offer if the job pays more than $80,000 a year. In a decision tree, this first consideration or question is represented as a decision node. Since the answer to this question is the most indicative of whether or not we will accept an offer, it is the first decision node in our tree and is thus known as the root node. The possible responses to the question are represented by a branch for yes and a branch for no. And depending on the answer, we either accept a job or reject it. The outcome of each response, accept or reject, is represented by a leaf or terminal node. \n",
    "\n",
    "<img src=\"img/decision-tree_01.png\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462ff1df",
   "metadata": {},
   "source": [
    "Now let's assume that commute time is also an important consideration as to whether we accept a job offer or not. Specifically, even if a job pays more than $80,000 a year, we may not be willing to take it if the commute is longer than an hour from where we live. In a decision tree, we model this by adding a second decision node to the tree. We now have two decision nodes to possibly consider before getting to the leaf nodes of accept or reject. \n",
    "\n",
    "<img src=\"img/decision-tree_02.png\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8361f5",
   "metadata": {},
   "source": [
    "Next, let's assume that paid time off is also very important to us. Specifically, we would accept an offer that pays less than $80,000 a year as long as it offered more than four weeks of paid leave. We model this question with a third decision node. We now have three decision nodes to possibly consider before getting to the leaf nodes of accept or reject. As more considerations come to mind, we can add them as subsequent decision nodes which would increase the size of our tree. However, if we decide that these three considerations are adequate, then this tree serves as a predictive model for whether or not we will accept a given job offer. This structure of a decision tree can be interpreted as a set of rules or guiding principles. For example, if we provide this model to a recruiter, they could easily follow the logic of this tree from the root node to the leaf node to figure out whether any given job offer would be acceptable to us or not. \n",
    "\n",
    "<img src=\"img/decision-tree_03.png\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e0d90d",
   "metadata": {},
   "source": [
    "Now imagine for a moment what your job acceptance decision tree could look like. I'm sure it would have more decision nodes than this one. I previously mentioned that decision trees are one of the most widely used machine learning approaches. A major reason for this is the ease with which decision trees can be translated into simple and understandable if-then-else rules. This makes them well suited for situations where transparency is important for **legal or compliance purposes** and for situations where the decision logic needs to be shared with non-technical stakeholders. As a supervised machine learning approach, decision trees can be used to solve **both classification problems and regression problems**. If our dependent variable is a categorical or discreet value such as color, true or false, yes or no, then the type of tree we build is called a classification tree. However, if our dependent variable is a continuous value, such as age, income, temperature, then the type of tree we build is called a regression tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b54d4a",
   "metadata": {},
   "source": [
    "## How is it built?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068d473a",
   "metadata": {},
   "source": [
    "Classification trees are built using a process known as recursive partitioning. The basic idea behind this process is to repeatedly split data into smaller subsets in such a way that maximizes the **homogeneity or similarity** of items within each subset. \n",
    "\n",
    "To illustrates how recursive partitioning helps us build a classification tree, let's imagine that we work for a small commercial bank and that we have historical data for 30 personal loans issued by our bank. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f705ed",
   "metadata": {},
   "source": [
    "Each loan record includes the annual income of the borrower, the amount that was borrowed, and the outcome of the loan, which is represented here by the default column. Note that the income and loan amount columns are what we call the independent variables or predictors while the default column is a dependent variable or class. \n",
    "\n",
    "<img src=\"img/decision-tree_04.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1748d8",
   "metadata": {},
   "source": [
    "Each of the 30 loans previously issued by our bank can be represented in terms of the dependent and independent variables this way using a scatter plot. From the plot, we can see that of the 30 loans in the dataset, 16 ended in default, the red triangles, and 14 were paid back in full, the green circles. \n",
    "\n",
    "<img src=\"img/decision-tree_05.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df093de",
   "metadata": {},
   "source": [
    "Recall that the idea behind recursive partitioning is to repeatedly split data into smaller subsets in such a way that maximizes the similarity of items within each subset. So the first thing we need to do here is to figure out how best to split this data into two so that we have partitions or subsets that maximize the **similarity or purity** of outcomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934fedd",
   "metadata": {},
   "source": [
    "Using two axis parallel lines, we scan both axes to determine where to split the data. By visual inspection, we find that splitting on the loan amount of $40,000 gives us the best split. \n",
    "\n",
    "<img src=\"img/decision-tree_06.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cd8d13",
   "metadata": {},
   "source": [
    "Based on the split, we get 14 loans of $40,000 or less to the left and 16 loans of more than $40,000 to the right. Splitting the data this way gives us the two partitions with the most homogeneity of loans in favor of one of the two outcomes. Any other axis parallel line we could have drawn would result in partitions with less purity or homogeneity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2485cbbd",
   "metadata": {},
   "source": [
    "Notice that I use the terms **homogeneity, similarity, and purity** to represent the same idea. This initial split creates the logic for the root node of our classification tree, which is shown here. It simply asks the question, did a customer borrow $40,000 or less? To create the branches and the next set of nodes, we make some generalizations or simplifying assumptions. Of the loans which were more than $40,000, 10 resulted in default while six were paid back in full. In other words, 63% of the loans, or 10 out of 16 loans, in this partition resulted in default. Because default is the dominant outcome in this partition, we will assume or generalize that any future loans that are for more than $40,000 will also result in default. As you can see, some of the loans in the partition, the red circles, do not conform to our assumption. We refer to these as the **misclassified examples** in the training data. \n",
    "\n",
    "<img src=\"img/decision-tree_07.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3839a330",
   "metadata": {},
   "source": [
    "Our goal should be to have very few of these. The assumption we made for this partition determines the structure of the first branch and leaf node in our classification tree. Now let's take a look at the other partition. Of the loans which were $40,000 or less, eight out of 14, or 57% were paid back in full while six resulted in default. Because not default is the dominant outcome in this partition, we generalize that any future loans that are $40,000 or less will also be paid back in full. As expected, we also have some misclassified examples in this partition. These are the green triangles. The generalization we made for this partition determines the structure of the second branch and leaf node in our classification tree. We can stop the recursive partitioning process here or we can decide to keep trying to create purer partitions within the data. For instance, we know that within the left partition, we misclassified six of the 14 examples. \n",
    "\n",
    "\n",
    "<img src=\"img/decision-tree_08.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e41ceb1",
   "metadata": {},
   "source": [
    "To reduce the number of misclassified examples, we need to further partition the data. Using two axis parallel lines, we scan to determine where to split this partition. By visual inspection, we find that splitting on an annual income of $20,000 gives us the best split. Of the eight customers who borrowed $40,000 or less and earn more than $20,000 a year, seven paid their loan back in full and one defaulted. Because not default is the dominant outcome in this partition, we generalize that any future customers who earn more than $20,000 a year and borrow $40,000 or less will also pay back their loan in full. In similar fashion, we generalize that any future customers who earn less than $20,000 a year and borrow $40,000 or less will default on their loan. \n",
    "\n",
    "<img src=\"img/decision-tree_09.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79b0304",
   "metadata": {},
   "source": [
    "Note that each of the partitions now only have one misclassified example. They are much purer. These two new partitions and the generalizations we made for them result in a structural change to our classification tree. The tree will now include a new decision node which branches into two new leaf nodes. \n",
    "\n",
    "<img src=\"img/decision-tree_10.png\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416325d5",
   "metadata": {},
   "source": [
    "We can continue the recursive partitioning process in an attempt to create smaller and more homogenous partitions, or we can stop here. Generally, classification tree algorithms continue the recursive partitioning process until all of the instances within the partition are of the same class or value, or all the features in the dataset have been exhausted, or when some user-defined condition has been satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b53740",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
